{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.99638755]\n",
      " [-2.29043125]]\n",
      "[[0.88041727]\n",
      " [0.09191855]]\n",
      "[[-1.]\n",
      " [-0.]]\n",
      "c: [[0.02274905]]\n"
     ]
    }
   ],
   "source": [
    "# disregard - numpy testing and initial attempt to make a cost function\n",
    "\n",
    "#xor\n",
    "training = [[0, 0], [1, 0], [0, 1], [1, 1]]\n",
    "results = [0, 1, 1, 0]\n",
    "\n",
    "weights1 = np.random.uniform(low=-1, high=1, size=(3, 2))  # looks like its 3 rows and 2 columns\n",
    "weights2 = np.random.uniform(low=-1, high=1, size=(1, 2))  \n",
    "\n",
    "bias1 = np.random.uniform(low=-1, high=1, size=3)\n",
    "bias2 = np.random.uniform(low=-1, high=1, size=1)\n",
    "print(\"weights1:        \",weights1)\n",
    "print(\"weights1.size:    \",weights1.size)   # the total number of elements in all rows and columns\n",
    "print(\"weights1.shape:   \",weights1.shape)  # returns a tuple that with number of rows, cols, etc... up to rank of \n",
    "print(\"weights1.T.shape: \",weights1.T.shape)\n",
    "\n",
    "# bias is a one dimentional vector\n",
    "#could use a Nx1 matrix or a N vector  pick whichever is easier when implementing\n",
    "print(\"bias1:        \",bias1)\n",
    "print(\"bias1.size:   \",bias1.size)\n",
    "print(\"bias1.shape   \",bias1.shape)\n",
    "print(\"bias1.T:      \",bias1.T    )  #transpose?  will it do anything to the actual 1D vector?\n",
    "print(\"bias1.T.shape \",bias1.T.shape)  #shape still (N,)\n",
    "print(\"bias dot bias \", np.dot(bias1,bias1))   # works fine without having to take transpose\n",
    "\n",
    "z1 = np.dot(weights1, training[0]) + bias1\n",
    "print(\"z1 shape \",z1.shape,\" contents:\" ,z1)\n",
    "\n",
    "\n",
    "print(\"test of forward for training sample 0: \", z1)\n",
    "layer1 = sigmoid(z1)\n",
    "    \n",
    "\n",
    "def feed_forward(weights,inputs):\n",
    "    # have a matrix for weights that's NxH (HxN)? where N is number of inputs and H is number of hidden nodes\n",
    "    # result of this function is matrix multiplying the weights by the input \n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "  return (x*(1-x)) if derivative else 1/(1+np.exp(-x))\n",
    "\n",
    "weights1 = np.random.uniform(low=-1, high=1, size=(16,16))\n",
    "z1 = np.dot(weights1, top_training[0])\n",
    "layer1 = np.maximum(z1, 0)\n",
    "layer1 = sigmoid(z1)\n",
    "\n",
    "#print(layer1)\n",
    "\n",
    "weights2 = np.random.uniform(low=-1, high=1, size=(2,16))\n",
    "z2 = np.dot(weights2, layer1)\n",
    "print(z2)\n",
    "\n",
    "layer2 = np.maximum(z2, 0)\n",
    "layer2 = sigmoid(z2)\n",
    "\n",
    "print(layer2)\n",
    "y = np.zeros((2, 1))\n",
    "y[0,0] = 1\n",
    "print(-y)\n",
    "def cost_function(v, y):\n",
    "    c = (np.dot((v -y).T, v - y))\n",
    "    print('c:', c)\n",
    "    \n",
    "cost_function(layer2, y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-sigmoid z1 [[-0.91435311]\n",
      " [ 0.82106279]\n",
      " [ 0.73319569]]\n",
      "layer 1 [[0.28610988]\n",
      " [0.69446189]\n",
      " [0.67550615]]\n",
      "test of forward for training sample 0 output:  [[0.50712605]]\n",
      "error: [[0.50712605]]\n",
      "er2: [[0.12675576]]\n",
      "er1 on right side of sigmoid [[ 0.11724132]\n",
      " [-0.04222791]\n",
      " [ 0.02569585]]\n",
      "weights1 [[-0.55846779 -0.56347274]\n",
      " [ 0.06770019  0.80640194]\n",
      " [-0.1061895   0.70429601]]\n",
      "sig weights to test on a matrix  [[-0.87035405 -0.88097427]\n",
      " [ 0.06311688  0.15611785]\n",
      " [-0.11746571  0.20826314]]\n",
      "layer1 [[0.28610988]\n",
      " [0.69446189]\n",
      " [0.67550615]]\n",
      "1-layer1 [[0.71389012]\n",
      " [0.30553811]\n",
      " [0.32449385]]\n",
      "layer1*layer1 [[0.08185886]\n",
      " [0.48227732]\n",
      " [0.45630856]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (3,1) and (3,1) not aligned: 1 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-316-811d0119729f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1-layer1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"layer1*layer1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlayer1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"layer1*(1-layer1)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sigmoid(layer1,false)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sigmoid(layer1,true)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;31m# This promotes 1-D vectors to row vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__rmul__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (3,1) and (3,1) not aligned: 1 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "#xor\n",
    "#  note that this section is incomplete, got stuck with an operation in numpy,  ran into some weirdness\n",
    "#  eventually figured out that changing '*' to np.multiply let things work for shapes (3,1)  i.e. 2d, but only 1 wide\n",
    "#  this section was suspended in favor of the implementation further down the page that sucessfully learns xor\n",
    "\n",
    "training = [[0, 0], [1, 0], [0, 1], [1, 1]]\n",
    "results = [[0], [1], [1], [0]]\n",
    "\n",
    "weights1 = np.random.uniform(low=-1, high=1, size=(3, 2))  # looks like its 3 rows and 2 colums (am i correct on this?)\n",
    "weights2 = np.random.uniform(low=-1, high=1, size=(1, 3))  \n",
    "\n",
    "bias1 = np.random.uniform(low=-1, high=1, size=(3, 1))\n",
    "bias2 = np.random.uniform(low=-1, high=1, size=(1, 1))\n",
    "\n",
    "z1 = np.dot(weights1, np.matrix(training[0]).T) + bias1\n",
    "layer1 = sigmoid(z1)\n",
    "print(\"pre-sigmoid z1\", z1)\n",
    "print(\"layer 1\", layer1)\n",
    "z2 = np.dot(weights2, z1) + bias2\n",
    "layer2 = sigmoid(z2)\n",
    "print(\"test of forward for training sample 0 output: \", layer2)\n",
    "\n",
    "er_out = layer2 - results[0]\n",
    "print(\"error:\", er_out)\n",
    "\n",
    "er2 = sigmoid(layer2,True) * er_out  # elementwise multiply with error in output   propogates backward\n",
    "\n",
    "print(\"er2:\",er2)\n",
    "\n",
    "er1 = np.dot(weights2.T,er2)  # keep it as a column v\n",
    "\n",
    "print(\"er1 on right side of sigmoid\",er1)\n",
    "print(\"weights1\",weights1)\n",
    "print(\"sig weights to test on a matrix \", sigmoid(weights1,True))\n",
    "#print(\"test sig er1  \", sigmoid(er1,True))\n",
    "\n",
    "print(\"layer1\",layer1)\n",
    "print(\"1-layer1\",1-layer1)\n",
    "print(\"layer1*layer1\",np.multiply(layer1 , layer1))\n",
    "print(\"layer1*(1-layer1)\",layer1 * (1-layer1))\n",
    "print(\"sigmoid(layer1,false)\",sigmoid(layer1,False))\n",
    "print(\"sigmoid(layer1,true)\",sigmoid(layer1,True))\n",
    "\n",
    "er1s = sigmoid(layer1,True) * er1\n",
    "print(\"er1 pre sigmoid\", er1s)\n",
    "\n",
    "def cost_function(v, y):\n",
    "    c = (np.dot((v -y).T, v - y))\n",
    "    print('c:', c[0,0])\n",
    "    \n",
    "#cost_function(layer2, results[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new weight is:  1.723239872027639   with rms: 0.2730036852103631\n",
      "new weight is:  2.3076091518999435   with rms: 0.1565102392533395\n",
      "new weight is:  2.641340728037396   with rms: 0.09042270384646102\n",
      "new weight is:  2.8319338491123545   with rms: 0.05342032778337136\n",
      "new weight is:  2.9407810197083166   with rms: 0.03343317340927186\n",
      "new weight is:  3.0029433185359276   with rms: 0.02348600308085217\n",
      "new weight is:  3.0384440244741358   with rms: 0.01915542023336074\n",
      "new weight is:  3.058718373169081   with rms: 0.01751291501075985\n",
      "new weight is:  3.070296994048332   with rms: 0.01694280778680697\n",
      "new weight is:  3.0769095103605837   with rms: 0.016752670926432282\n"
     ]
    }
   ],
   "source": [
    "#single dimensional simple example of iterative improvement of our 1x1 weight w\n",
    "\n",
    "import math\n",
    "\n",
    "inputs   = [ 0, .1 , .2 , .3,  -.1 , -.2 , -.3 ]\n",
    "expected = [ 0, .3 , .6 , .96 , -.3 , -.6 , -.92 ]\n",
    "w= .7\n",
    "alpha = 0.01  # learning rate  small enough to not diverge\n",
    "\n",
    "def forward(x):\n",
    "    global w\n",
    "    return w * x\n",
    "\n",
    "def train_sample(x, p):\n",
    "    global w\n",
    "    global alpha\n",
    "    y = forward(x)\n",
    "    e = y - p\n",
    "    w -= x * e * alpha \n",
    "    \n",
    "def rms():  # root mean square error (the error that will be minimized by gradient descent)\n",
    "    s=0\n",
    "    for i in range(len(inputs)):\n",
    "        e=forward(inputs[i])-expected[i]\n",
    "        s+= e*e;\n",
    "    return math.sqrt(s/len(inputs))  #  \n",
    "        \n",
    "\n",
    "    \n",
    "def train_batch():\n",
    "    global w\n",
    "    for k in range(10):\n",
    "        for j in range(200):\n",
    "            for i in range(len(inputs)):\n",
    "                train_sample(inputs[i], expected[i])\n",
    "        print(\"new weight is: \", w, \"  with rms:\" ,rms())\n",
    "        \n",
    "        \n",
    "train_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret hidden thing that we are trying to learn is a matrix (2, 2)  values:   [[-1  1]\n",
      " [-1 -1]]\n",
      "our starting random guess at our weights: [[-0.39994677  0.04331311]\n",
      " [ 0.01494271  0.48366034]]\n",
      "new weight is:  [[-0.68918556  0.53843479]\n",
      " [-0.44345828 -0.26172642]]\n",
      "new weight is:  [[-0.82559289  0.75704504]\n",
      " [-0.69993639 -0.61907099]]\n",
      "new weight is:  [[-0.92223028  0.88029464]\n",
      " [-0.82366027 -0.78166408]]\n",
      "new weight is:  [[-0.95807754  0.92967197]\n",
      " [-0.90835308 -0.87494911]]\n",
      "new weight is:  [[-0.97590999  0.96040664]\n",
      " [-0.95588819 -0.93486621]]\n",
      "new weight is:  [[-0.98739715  0.98138109]\n",
      " [-0.97692236 -0.96929588]]\n",
      "new weight is:  [[-0.99458297  0.99032961]\n",
      " [-0.98894398 -0.98343913]]\n",
      "new weight is:  [[-0.99695438  0.99498542]\n",
      " [-0.99458289 -0.99188144]]\n",
      "new weight is:  [[-0.99824216  0.99737769]\n",
      " [-0.99737432 -0.99605798]]\n",
      "new weight is:  [[-0.99922765  0.99869801]\n",
      " [-0.99856731 -0.99784814]]\n"
     ]
    }
   ],
   "source": [
    "# multidimensional test with no hidden layers\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "actual = np.array([[-1,1],[-1,-1]])\n",
    "print(\"secret hidden thing that we are trying to learn is a matrix\", actual.shape, \" values:  \",actual)\n",
    "\n",
    "weights = np.random.uniform(low=-1, high=1, size=(2, 2))\n",
    "print(\"our starting random guess at our weights:\",weights)\n",
    "\n",
    "\n",
    "alpha = 0.01  # learning rate  small enough to not diverge\n",
    "\n",
    "def forward(x):\n",
    "    global weights\n",
    "    return np.matmul(weights, x)\n",
    "\n",
    "def train_sample(x, p):\n",
    "    global weights\n",
    "    global alpha\n",
    "    y = forward(x)\n",
    "    e = y - p\n",
    "    weights = weights - np.outer(e , x ) * alpha        \n",
    "\n",
    "    \n",
    "def train_batch():\n",
    "    for k in range(10):\n",
    "        for j in range(200):\n",
    "            x = np.random.uniform(low=-1, high=1, size=(2,))\n",
    "            p= np.matmul(actual, x)\n",
    "            train_sample(x,p)\n",
    "        print(\"new weight is: \", weights)\n",
    "        \n",
    "        \n",
    "train_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret hidden thing that we are trying to learn is a matrix (2, 2)  values:   [[-1  1]\n",
      " [-1 -1]]\n",
      "our starting random guess at our weights: [[ 8.73410442e-01 -6.62349523e-04]\n",
      " [ 2.78908292e-01 -2.83739673e-01]]\n",
      "second layer of weights that we already know:  (2, 2)  values:   [[ 0.5 -2. ]\n",
      " [ 0.5 -1. ]]\n",
      "new weights1 is:  [[ 0.52137448  0.45236998]\n",
      " [-0.53936487 -1.16580903]]\n",
      "new weights1 is:  [[ 0.10707745  0.6121484 ]\n",
      " [-0.66480392 -1.11743202]]\n",
      "new weights1 is:  [[-0.18310284  0.70802608]\n",
      " [-0.75266344 -1.08840259]]\n",
      "new weights1 is:  [[-0.39642722  0.78668819]\n",
      " [-0.81725287 -1.06458562]]\n",
      "new weights1 is:  [[-0.55734467  0.84313484]\n",
      " [-0.86597475 -1.04749495]]\n",
      "new weights1 is:  [[-0.66817876  0.88310188]\n",
      " [-0.89953261 -1.0353939 ]]\n",
      "new weights1 is:  [[-0.75654606  0.91195678]\n",
      " [-0.92628808 -1.02665734]]\n",
      "new weights1 is:  [[-0.82050231  0.93674543]\n",
      " [-0.94565247 -1.01915194]]\n",
      "new weights1 is:  [[-0.86744607  0.9524128 ]\n",
      " [-0.9598659  -1.01440824]]\n",
      "new weights1 is:  [[-0.90180395  0.96469088]\n",
      " [-0.97026863 -1.01069074]]\n"
     ]
    }
   ],
   "source": [
    "# test where one weight matrix is known and one isn't (so it has to be adjusted)\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "actual = np.array([[-1,1],[-1,-1]])\n",
    "print(\"secret hidden thing that we are trying to learn is a matrix\", actual.shape, \" values:  \",actual)\n",
    "\n",
    "weights1 = np.random.uniform(low=-1, high=1, size=(2, 2))\n",
    "print(\"our starting random guess at our weights:\",weights1)\n",
    "\n",
    "weights2 = np.array([[0.5,-2],[0.5,-1]])\n",
    "print(\"second layer of weights that we already know: \", weights2.shape, \" values:  \",weights2)\n",
    "\n",
    "\n",
    "alpha = 0.01  # learning rate small enough to not diverge\n",
    "\n",
    "def forward(x):\n",
    "    out1 = np.matmul(weights1, x) #after first set of weights applied\n",
    "    return np.matmul(weights2, out1)\n",
    "\n",
    "def train_sample(x, p):\n",
    "    global weights1\n",
    "    global alpha\n",
    "    y = forward(x)\n",
    "    e = y - p\n",
    "    e1 = np.matmul(e,weights2) #derivative of weights2 times x is just weights2\n",
    "    weights1 = weights1 - np.outer(e1 , x) * alpha \n",
    "        \n",
    "    \n",
    "def train_batch():\n",
    "    for k in range(10):\n",
    "        for j in range(2000):\n",
    "            x = np.random.uniform(low=-1, high=1, size=(2,))\n",
    "            p1 = np.matmul(actual, x)\n",
    "            p2 = np.matmul(weights2, p1)\n",
    "            train_sample(x,p2)\n",
    "        print(\"new weights1 is: \", weights1)\n",
    "        \n",
    "        \n",
    "train_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret hidden thing that we are trying to learn is a matrix (4, 3)  values:   [[-0.90397983 -0.61630678  0.13581379]\n",
      " [ 0.99052838  0.35368045 -0.54716969]\n",
      " [ 0.94569683 -0.48596754 -0.6675901 ]\n",
      " [-0.91770213 -0.87955419 -0.33859548]]\n",
      "our starting random guess at our weights: [[ 0.08653376  0.2502532  -0.52304101]\n",
      " [-0.87028645  0.94557717 -0.72706043]\n",
      " [-0.70264222  0.70918523  0.26488839]\n",
      " [-0.05445232  0.20887784  0.5895079 ]]\n",
      "second layer of weights that we already know:  (4, 4)  values:   [[ 0.821859   -0.61120305 -0.07763283 -0.63417268]\n",
      " [ 0.84887314 -0.9439364   0.71505067 -0.6764717 ]\n",
      " [ 0.12034138  0.56166452  0.83373274  0.77928986]\n",
      " [-0.33154084  0.07462603 -0.08422212  0.76383978]]\n",
      "new weights1-actual is:  [[ 0.01592282  0.01033042 -0.00646391]\n",
      " [ 0.00583872  0.00366667 -0.00344019]\n",
      " [-0.00948142 -0.00449051  0.00457553]\n",
      " [ 0.00706919  0.00465002 -0.00159184]]\n",
      "new weights1-actual is:  [[ 3.64807000e-04  2.47178855e-04 -1.21944452e-04]\n",
      " [ 1.50388868e-04  9.51682234e-05 -7.24382314e-05]\n",
      " [-2.08819999e-04 -1.14574668e-04  8.21795768e-05]\n",
      " [ 1.46132158e-04  1.03166911e-04 -2.54331607e-05]]\n",
      "new weights1-actual is:  [[ 8.12275666e-06  5.91059276e-06 -2.78129908e-06]\n",
      " [ 3.36768004e-06  2.29570287e-06 -1.64427650e-06]\n",
      " [-4.70157114e-06 -2.79005959e-06  1.88589142e-06]\n",
      " [ 3.23544752e-06  2.46725058e-06 -5.91736821e-07]]\n",
      "new weights1-actual is:  [[ 1.82178719e-07  1.32055425e-07 -5.49605265e-08]\n",
      " [ 7.48918891e-08  5.02860250e-08 -3.40350340e-08]\n",
      " [-1.02273148e-07 -6.11433190e-08  3.63739014e-08]\n",
      " [ 7.32819992e-08  5.57821912e-08 -1.02601473e-08]]\n",
      "new weights1-actual is:  [[ 4.16492341e-09  3.12449200e-09 -1.21746663e-09]\n",
      " [ 1.70983439e-09  1.20001703e-09 -7.59604935e-10]\n",
      " [-2.36508535e-09 -1.47115325e-09  8.31343328e-10]\n",
      " [ 1.69914638e-09  1.32060596e-09 -2.13849605e-10]]\n",
      "new weights1-actual is:  [[ 9.50159951e-11  7.06694703e-11 -3.00745262e-11]\n",
      " [ 3.93600708e-11  2.71757061e-11 -1.83859594e-11]\n",
      " [-5.40694156e-11 -3.33713612e-11  2.06925588e-11]\n",
      " [ 3.80578902e-11  2.97254443e-11 -5.66602321e-12]]\n",
      "new weights1-actual is:  [[ 2.09698925e-12  1.59594560e-12 -6.80067114e-13]\n",
      " [ 8.67972361e-13  6.11677375e-13 -4.21995772e-13]\n",
      " [-1.19848576e-12 -7.47568674e-13  4.60298466e-13]\n",
      " [ 8.43214387e-13  6.72351064e-13 -1.31561428e-13]]\n",
      "new weights1-actual is:  [[ 9.15933995e-14  6.46149800e-14 -2.61457522e-14]\n",
      " [ 4.76285678e-14  1.58761893e-14 -2.45359288e-14]\n",
      " [-4.49640325e-14 -3.10862447e-14  1.66533454e-14]\n",
      " [ 3.37507799e-14  3.64153152e-14  2.49800181e-15]]\n",
      "new weights1-actual is:  [[ 4.28546088e-14  3.03090886e-14 -1.61815006e-14]\n",
      " [ 2.62012634e-14  4.99600361e-16 -1.62092562e-14]\n",
      " [-2.02060590e-14 -1.59317004e-14  1.19904087e-14]\n",
      " [ 1.49880108e-14  2.43138842e-14  2.49800181e-15]]\n",
      "new weights1-actual is:  [[ 3.51940699e-14  2.44249065e-14 -1.57651669e-14]\n",
      " [ 2.20934382e-14 -3.33066907e-16 -1.52100554e-14]\n",
      " [-1.72084569e-14 -1.25455202e-14  1.19904087e-14]\n",
      " [ 1.27675648e-14  2.09832152e-14  1.99840144e-15]]\n"
     ]
    }
   ],
   "source": [
    "# 3 - 4 - 4\n",
    "# 2nd layer is still locked (unchanging), \n",
    "# added a sigmoid layer between the two fully connected layers\n",
    "# note that more training time is required to converge\n",
    "# or learning rate, alpha, could be increased (from 0.01 to 0.1)\n",
    "#\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "  return (np.multiply(x,(1-x))) if derivative else 1/(1+np.exp(-x))\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "actual = np.random.uniform(low=-1, high=1, size=(4, 3))\n",
    "print(\"secret hidden thing that we are trying to learn is a matrix\", actual.shape, \" values:  \",actual)\n",
    "\n",
    "weights1 = np.random.uniform(low=-1, high=1, size=(4, 3))\n",
    "print(\"our starting random guess at our weights:\",weights1)\n",
    "\n",
    "weights2 = np.random.uniform(low=-1, high=1, size=(4, 4))\n",
    "print(\"second layer of weights that we already know: \", weights2.shape, \" values:  \",weights2)\n",
    "\n",
    "out1 = 0\n",
    "out2 = 0\n",
    "\n",
    "\n",
    "alpha = 0.1  # learning rate small enough to not diverge\n",
    "#might need to normalize data so this isn't too big\n",
    "\n",
    "def forward(x):\n",
    "    global out1, out2 # need to keep these global since they are used for backprop\n",
    "    out1 = np.matmul(weights1, x) #after first set of weights applied\n",
    "    out2 = sigmoid(out1)\n",
    "    return np.matmul(weights2, out2)\n",
    "\n",
    "def train_sample(x, p):\n",
    "    global weights1\n",
    "    global alpha\n",
    "    y = forward(x)\n",
    "    e = y - p\n",
    "    e2 = np.matmul(e, weights2) #derivative of weights2 times x is just weights2\n",
    "    e1 = np.multiply(e2,sigmoid(out2, True))\n",
    "    weights1 = weights1 - np.outer(e1 , x) * alpha \n",
    "    \n",
    "def rms():  # root mean square error (the error that will be minimized by gradient descent)\n",
    "    s=0\n",
    "    for i in range(len(inputs)):\n",
    "        e=forward(inputs[i])-expected[i]\n",
    "        s+= e*e;\n",
    "    return math.sqrt(s/len(inputs))  #  \n",
    "        \n",
    "\n",
    "    \n",
    "def train_batch():\n",
    "    for k in range(10):\n",
    "        for j in range(20000):\n",
    "            x = np.random.uniform(low=-1, high=1, size=(3,))\n",
    "            p1 = np.matmul(actual, x)\n",
    "            p2 = sigmoid(p1)\n",
    "            p3 = np.matmul(weights2, p2)\n",
    "            train_sample(x,p3)\n",
    "        print(\"new weights1-actual is: \", weights1-actual)\n",
    "        \n",
    "        \n",
    "train_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.54655761  0.71660454  0.83936672]]\n",
      "[[1.54655761 0.28339546 0.16063328]]\n",
      "[[-0.84528283  0.20308247  0.13483023]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.uniform(low=-1, high=1, size=(1,3))\n",
    "print(x)\n",
    "print(1 - x)\n",
    "print(x * (1-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret hidden thing that we are trying to learn is a matrix (3, 2)  values:   [[ 0.97449584 -0.68847914]\n",
      " [-0.38264094 -0.60920227]\n",
      " [ 0.09154447  0.34960532]]\n",
      "our starting random guess at our weights: [[ 0.999307   -0.97004213]\n",
      " [ 0.33663918  0.34674273]\n",
      " [-0.90668574 -0.68351756]]\n",
      "second layer of weights that we also have to learn know:  (2, 3)  values:   [[-0.60020535 -0.62542887 -0.17848813]\n",
      " [-0.66861081 -0.7604566   0.365913  ]]\n",
      "rmse:  0.03945318498554707\n",
      "rmse:  0.00212229097305698\n",
      "rmse:  0.002033862912913282\n",
      "rmse:  0.001944997874242392\n",
      "rmse:  0.001861638336127047\n",
      "rmse:  0.0017747907140690602\n",
      "rmse:  0.0017214911833457415\n",
      "rmse:  0.001655888434810225\n",
      "rmse:  0.0015950831045763048\n",
      "rmse:  0.0015445932715550288\n",
      "rmse:  0.0014975651694478528\n",
      "rmse:  0.0014522639346611257\n",
      "rmse:  0.001410290770220825\n",
      "rmse:  0.0013815672349808612\n",
      "rmse:  0.0013504573729499966\n",
      "rmse:  0.0013226426501054774\n",
      "rmse:  0.001294965625810375\n",
      "rmse:  0.0012714871564854788\n",
      "rmse:  0.001257357480905463\n",
      "rmse:  0.0012385120800695705\n",
      "rmse:  0.0012186548623041144\n",
      "rmse:  0.0012040819805044535\n",
      "rmse:  0.0011906663610389491\n",
      "rmse:  0.001174694482181214\n",
      "rmse:  0.0011658606048340603\n",
      "rmse:  0.0011565966051232089\n",
      "rmse:  0.0011465621749317968\n",
      "rmse:  0.0011339911426268548\n",
      "rmse:  0.001126752458606649\n",
      "rmse:  0.0011141003982882858\n"
     ]
    }
   ],
   "source": [
    "# 2 -3- 2\n",
    "# 2nd layer is unknown and must be learned.   not locked\n",
    "# this  now has all the ingredients of a a neural network with input, output and a hidden layer\n",
    "# only using sigmoid after the first fully connected layer, not at the end after 2nd fully connected \n",
    "#\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "  return (np.multiply(x,(1-x))) if derivative else 1/(1+np.exp(-x))\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "actual = np.random.uniform(low=-1, high=1, size=(3, 2))\n",
    "print(\"secret hidden thing that we are trying to learn is a matrix\", actual.shape, \" values:  \",actual)\n",
    "\n",
    "actual2 = np.random.uniform(low=-1,high=1,size=(2,3))\n",
    "\n",
    "weights1 = np.random.uniform(low=-1, high=1, size=(3, 2))\n",
    "print(\"our starting random guess at our weights:\",weights1)\n",
    "\n",
    "weights2 = np.random.uniform(low=-1, high=1, size=(2, 3))\n",
    "print(\"second layer of weights that we also have to learn know: \", weights2.shape, \" values:  \",weights2)\n",
    "\n",
    "out1 = 0\n",
    "out2 = 0\n",
    "\n",
    "\n",
    "alpha = 0.025  # learning rate small enough to not diverge\n",
    "#might need to normalize data so this isn't too big\n",
    "\n",
    "def forward(x):\n",
    "    global out1, out2\n",
    "    out1 = np.matmul(weights1, x) #after first set of weights applied\n",
    "    out2 = sigmoid(out1)\n",
    "    return np.matmul(weights2, out2)\n",
    "\n",
    "def train_sample(x, p):\n",
    "    global weights1, weights2\n",
    "    global alpha\n",
    "    y = forward(x)\n",
    "    e = y - p\n",
    "    e2 = np.matmul(e, weights2) #derivative of weights2 times x is just weights2\n",
    "    e1 = np.multiply(e2,sigmoid(out2, True))\n",
    "    weights2 = weights2 - np.outer(e, out2) * alpha\n",
    "    weights1 = weights1 - np.outer(e1 , x) * alpha \n",
    "    return np.dot(e,e)\n",
    "    \n",
    "def rms():  # root mean square error (the error that will be minimized by gradient descent)\n",
    "    s=0\n",
    "    for i in range(len(inputs)):\n",
    "        e=forward(inputs[i])-expected[i]\n",
    "        s+= e*e;\n",
    "    return math.sqrt(s/len(inputs))  #  \n",
    "        \n",
    "\n",
    "    \n",
    "def train_batch():\n",
    "    batchsize = 50000\n",
    "    for k in range(30):\n",
    "        sse=0\n",
    "        for j in range(batchsize):\n",
    "            x = np.random.uniform(low=-1, high=1, size=(2,))\n",
    "            p1 = np.matmul(actual, x)\n",
    "            p2 = sigmoid(p1)\n",
    "            p3 = np.matmul(actual2, p2)\n",
    "            sse += train_sample(x,p3)\n",
    "        print(\"rmse: \",math.sqrt(sse/batchsize))\n",
    "        #print(\"new weights1-actual is: \", weights1-actual)\n",
    "        #print(\"new weights2-actual2 is: \", weights2-actual2)\n",
    "        \n",
    "        \n",
    "train_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new weights1-actual is:  [[ 0.24545558 -0.21447961]\n",
      " [ 0.91087338  1.05619956]\n",
      " [-0.72683636 -0.66523868]]\n",
      "new weights2-actual2 is:  [[ 0.41494286 -1.12372336  0.70877438]\n",
      " [-0.04375892  0.3780882  -0.33421679]]\n"
     ]
    }
   ],
   "source": [
    "print(\"new weights1-actual is: \", weights1-actual)\n",
    "print(\"new weights2-actual2 is: \", weights2-actual2)\n",
    "     \n",
    "#not exactly the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our starting random guess at our weights: [[-0.84641648  0.88176502]\n",
      " [-0.49487212  0.12773519]\n",
      " [-0.46190939 -0.4921082 ]]\n",
      "second layer of weights that we also have to learn know:  (1, 3)  values:   [[ 0.19911986 -0.6669408   0.65966623]]\n",
      "rmse:  0.8634098344491802\n",
      "rmse:  0.14485350042314146\n",
      "rmse:  0.07182242134494259\n",
      "rmse:  0.05374436380615415\n",
      "rmse:  0.04439602562924932\n",
      "rmse:  0.03842183772964676\n",
      "rmse:  0.03416851313067495\n",
      "rmse:  0.030931879530190477\n",
      "rmse:  0.028354327781020032\n",
      "rmse:  0.026232420653636214\n",
      "new weights1 is:  [[-5.18005722  0.3517591 ]\n",
      " [-3.23767606 -3.07491984]\n",
      " [ 0.18269852 -4.99468673]]\n",
      "new weights2 is:  [[ 1.98493586 -4.07792348  2.0936063 ]]\n",
      "[0, 0]\n",
      "feed-forward for input:  [0, 0]  result:  [0.00030934]\n",
      "[1, 0]\n",
      "feed-forward for input:  [1, 0]  result:  [0.99923842]\n",
      "[0, 1]\n",
      "feed-forward for input:  [0, 1]  result:  [0.9992757]\n",
      "[1, 1]\n",
      "feed-forward for input:  [1, 1]  result:  [0.02525972]\n"
     ]
    }
   ],
   "source": [
    "# 2 -3- 1  (2 inputs, 3 hidden nodes, 1 output node)\n",
    "# xor  classic example of training a neural network to learn the exclusive-or function\n",
    "# note that xor is non-linear and thus cannot be done with only a single layer nor without any activation (sigmoid) layers\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "  return (np.multiply(x,(1-x))) if derivative else 1/(1+np.exp(-x))\n",
    "\n",
    "#xor\n",
    "training = [[0, 0], [1, 0], [0, 1], [1, 1]]\n",
    "results = [0, 1, 1, 0]   \n",
    "\n",
    "weights1 = np.random.uniform(low=-1, high=1, size=(3, 2))\n",
    "print(\"our starting random guess at our weights:\",weights1)\n",
    "\n",
    "weights2 = np.random.uniform(low=-1, high=1, size=(1, 3))\n",
    "print(\"second layer of weights that we also have to learn know: \", weights2.shape, \" values:  \",weights2)\n",
    "\n",
    "out1 = 0\n",
    "out2 = 0\n",
    "\n",
    "\n",
    "alpha = 0.025  # learning rate small enough to not diverge\n",
    "\n",
    "def forward(x):\n",
    "    global out1, out2\n",
    "    out1 = np.matmul(weights1, x) #after first set of weights applied\n",
    "    out2 = sigmoid(out1)\n",
    "    return np.matmul(weights2, out2)\n",
    "\n",
    "def train_sample(x, p):\n",
    "    global weights1, weights2\n",
    "    global alpha\n",
    "    y = forward(x)\n",
    "    e = y - p\n",
    "    e2 = np.matmul(e, weights2) #derivative of weights2 times x is just weights2\n",
    "    e1 = np.multiply(e2,sigmoid(out2, True)) #derivative of sigmoid uses the y as the input\n",
    "    weights2 = weights2 - np.outer(e, out2) * alpha # out2 is global from the feed forward\n",
    "    weights1 = weights1 - np.outer(e1 , x) * alpha \n",
    "    return np.dot(e,e) #return the error for this sample\n",
    "    \n",
    "    \n",
    "def train_batch():    \n",
    "    batchsize = 5000\n",
    "    for k in range(10):\n",
    "        sse=0\n",
    "        for j in range(batchsize):\n",
    "            for i in range(len(training)):\n",
    "                x = training[i]\n",
    "                p = [results[i]]\n",
    "                sse += train_sample(x,p)\n",
    "        print(\"rmse: \", math.sqrt(sse/batchsize))\n",
    "        #print(\"new weights1-actual is: \", weights1-actual)\n",
    "        #print(\"new weights2-actual2 is: \", weights2-actual2)\n",
    "     \n",
    "      \n",
    "train_batch()\n",
    "print(\"new weights1 is: \", weights1)\n",
    "print(\"new weights2 is: \", weights2)\n",
    "\n",
    "for x in training:\n",
    "    print(x)\n",
    "    print('feed-forward for input: ', x,' result: ', forward(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempted to make a 16 pixel input, hidden layer, and then 1 bit output. \n",
    "# If the image has dark pixels in the upper half, it is classified as a 1. \n",
    "# If it has dark pixels in the lower half it is a 0\n",
    "# gave up this since it's way to slow although it did compile!\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "# 16 pixels long\n",
    "\n",
    "training = []\n",
    "for i in range(50):\n",
    "    v = np.zeros(16)\n",
    "    for j in range(random.randint(1, 4)): #add up to 4 pixels\n",
    "        index = random.randint(1, 7)\n",
    "        v[index] = 1\n",
    "    training.append(v)\n",
    "\n",
    "for i in range(50):\n",
    "    v = np.zeros(16)\n",
    "    for j in range(random.randint(1, 4)): #add up to 4 pixels\n",
    "        index = random.randint(8, 15)\n",
    "        v[index] = 1\n",
    "    training.append(v)\n",
    "\n",
    "# 1 for top, 0 for bottom\n",
    "results = np.zeros(100)\n",
    "results[50:] = 1\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "  return (np.multiply(x,(1-x))) if derivative else 1/(1+np.exp(-x))\n",
    "\n",
    "weights1 = np.random.uniform(low=-1, high=1, size=(16, 16))\n",
    "#print(\"our starting random guess at our weights:\",weights1)\n",
    "\n",
    "weights2 = np.random.uniform(low=-1, high=1, size=(1, 16))\n",
    "#print(\"second layer of weights that we also have to learn know: \", weights2.shape, \" values:  \",weights2)\n",
    "\n",
    "out1 = 0\n",
    "out2 = 0\n",
    "\n",
    "\n",
    "alpha = 0.025  # learning rate small enough to not diverge\n",
    "#might need to normalize data so this isn't too big\n",
    "\n",
    "def forward(x):\n",
    "    global out1, out2\n",
    "    out1 = np.matmul(weights1, x) #after first set of weights applied\n",
    "    out2 = sigmoid(out1)\n",
    "    return np.matmul(weights2, out2)\n",
    "\n",
    "def train_sample(x, p):\n",
    "    global weights1, weights2\n",
    "    global alpha\n",
    "    y = forward(x)\n",
    "    e = y - p\n",
    "    e2 = np.matmul(e, weights2) #derivative of weights2 times x is just weights2\n",
    "    e1 = np.multiply(e2,sigmoid(out2, True))\n",
    "    weights2 = weights2 - np.outer(e, out2) * alpha\n",
    "    weights1 = weights1 - np.outer(e1 , x) * alpha \n",
    "    return np.dot(e,e)\n",
    "    \n",
    "    \n",
    "def train_batch():\n",
    "    batchsize = 5000\n",
    "    for k in range(10):\n",
    "        sse=0\n",
    "        for j in range(batchsize):\n",
    "            for i in range(len(training)):\n",
    "                x = training[i]\n",
    "                p = [results[i]]\n",
    "                sse += train_sample(x,p)\n",
    "        print(\"rmse: \", math.sqrt(sse/batchsize))\n",
    "        #print(\"new weights1-actual is: \", weights1-actual)\n",
    "        #print(\"new weights2-actual2 is: \", weights2-actual2)\n",
    "        \n",
    "        \n",
    "train_batch()\n",
    "print(\"new weights1 is: \", weights1)\n",
    "print(\"new weights2 is: \", weights2)\n",
    "\n",
    "for x in training:\n",
    "    print('feed-forward for input: ', x,' result: ', forward(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
